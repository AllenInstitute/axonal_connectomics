# Code for generating segmentation & skeleton layers for visualization in neuroglancer.
#
# Segmentation input format:
#   TIFF images stored in the directory ${INPUT}/Segmentation_labeled/
#   Filenames should be zero padded and named NNNN.tiff
#
# Skeleton input format:
#   SWC files stored in the directory ${INPUT}/swc_files_nm
#   Filenames should be zero padded and named NNNN.swc.
#   The integer value of NNNN should match the segmentation label in the segmentation TIFFs.
# 
# Example usage:
#   python segmentation.py --input_dir /Users/eric/nobackup/allen/olga/MN7_RH_3_2_S35_220127_high_res/Pos10_10 --output_dir /Users/eric/nobackup/allen/out/Pos10_10 
#
# TODO: Tweak compression & encoding forants; e.g., do we need uint64?
# TODO: Use the sharded format (fewer files)

import numpy as np
import tifffile
import glob
import os
import json
import logging

import argschema
import argschema.validate

from cloudvolume import CloudVolume, Skeleton

def generate_ngl_segmentation(source_path, out_path, chunk_size, resolution):
    """Create a neuroglancer precomputed segmentation volume from a tiff stack.
    
       source_path: directory underwhich `swc_files_nm` will be found
                    (Example path: /data/olga/MN7_RH_3_2_S35_220127_high_res/Pos10_10)
       out_path: directory where new the new cloud volume segmentation layer should be generated
                    (Example path: /data/out/MN7_RH_3_2_S35_220127_high_res/Pos10_10)
       chunk_size: list or array with chunk size for segmentation layer
                    (Example: (128,128,128))
       resolution: voxel resolution, in nm (passed on to neuroglancer via 'info' file)
                    (Example: (4,4,40))
    """

    # Read tiffstack into memory
    files = glob.glob(f"{source_path}/Segmentation_labeled/*.tif")
    files = sorted(files)
    data = None
    for layer in range(len(files)):
        image = tifffile.imread(files[layer])
        # Allocate array if needed; use the number of files and dimensions of first file
        if data is None:
            data = np.zeros(shape=(image.shape[1], image.shape[0], len(files)), dtype=image.dtype)
        data[:, :, layer] = image.T   # Tiff files have X & Y swapped
 
    info = CloudVolume.create_new_info(
        num_channels    = 1,
        layer_type      = 'segmentation',
        data_type       = 'uint64', # Channel images might be 'uint8'
        # raw, png, jpeg, compressed_segmentation, fpzip, kempressed, compresso
        encoding        = 'compressed_segmentation', 
        resolution      = resolution, # Voxel scaling, units are in nanometers
        voxel_offset    = [0, 0, 0], # x,y,z offset in voxels from the origin
        # Pick a convenient size for your underlying chunk representation
        # Powers of two are recommended, doesn't need to cover image exactly
        chunk_size      = chunk_size, # units are voxels
        volume_size     = data.shape, # e.g. a cubic millimeter dataset
        skeletons       = 'skeletons'
        )

    vol = CloudVolume(f'file://{out_path}', info=info, compress='', cache=False)
    logging.info(f"Creating cloud volume: {vol.info}")
    vol.commit_info()
    vol.commit_provenance()
    vol[:,:,:] = data.astype(np.uint64)


def generate_ngl_skeletons(source_path, out_path):
    """Generate skeletons from SWC files.
       This currently assumes the neuroglancer precomputed volume has already been generated by generate_ngl_segmentation.

       source_path: directory underwhich `swc_files_nm` will be found.
       out_path: directory with the previously generated segmentation layer.
    """

    vol = CloudVolume(f'file://{out_path}', compress='')
    # Remove vertex_attributes: vertex_types(uint8) breaks neuroglancer
    vol.skeleton.meta.info.pop("vertex_attributes", None)
    vol.skeleton.meta.commit_info()

    files = glob.glob(f"{source_path}/swc_files_nm/*.swc")
    files = sorted(files)
    skel_dir = os.path.join(out_path, "skeletons")
    if not os.path.exists(skel_dir):
        os.makedirs(skel_dir)
    
    segprops = {"@type": "neuroglancer_segment_properties",
            "inline" : {
                "ids" : [],
                "properties" : [
                    {"id": "tags",
                        "type": "tags",
                        "tags" : ["all"],
                        "values" : []
                    },
                    {"id": "length",
                        "type": "number",
                        "data_type" : "float32",
                        "values" : []
                    }
                ]},
            }
    
    for filename in files:
        # ..../NNNN.swc -> NNNN
        skel_id = int(os.path.split(filename)[-1].split('.')[0])
        with open(filename, mode='r') as f:
            swc = f.read()
        skel = Skeleton.from_swc(swc)
        skel.id = skel_id
 
        vol.skeleton.upload(skel)

        segprops["inline"]["ids"].append(str(skel_id))
        segprops["inline"]["properties"][0]["values"].append([0])  # tags
        segprops["inline"]["properties"][1]["values"].append(str(skel.cable_length()))
        
            
    # Write the segment properties
    segment_info_dir = os.path.join(out_path, "segment_properties")
    os.makedirs(segment_info_dir, exist_ok=True)
    with open(os.path.join(segment_info_dir, "info"), "w") as f:
        json.dump(segprops, f)
        
    # Re-write info file with added segment_properties
    with open(f'{out_path}/info', 'r') as f:
        infofile = json.load(f)
    infofile['segment_properties'] = 'segment_properties'
    with open(f'{out_path}/info', 'w') as f:
        json.dump(infofile, f)


class SegmentationToNeuroglancerParameters(argschema.ArgSchema):
    input_dir = argschema.fields.InputDir(required=True)
    output_dir = argschema.fields.OutputDir(required=True)

    chunk_size = argschema.fields.NumpyArray(dtype='int', required=False,
            default=[512, 512, 64], validate=argschema.validate.Shape((3,)))

    resolution = argschema.fields.NumpyArray(dtype='float', required=False,
        default=[406, 406, 1997.72], validate=argschema.validate.Shape((3,)))
   


class SegmentationToNeuroglancer(argschema.ArgSchemaParser):
    default_schema = SegmentationToNeuroglancerParameters

    def run(self):
        generate_ngl_segmentation(self.args["input_dir"], self.args["output_dir"],
            chunk_size=self.args["chunk_size"], resolution=self.args["resolution"])
        generate_ngl_skeletons(self.args["input_dir"], self.args["output_dir"])

if __name__ == "__main__":
    mod = SegmentationToNeuroglancer()
    mod.run()